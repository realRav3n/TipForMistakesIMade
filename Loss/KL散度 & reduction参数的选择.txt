class KLDivLoss(nn.Module):
    def __init__(self):
        super(KLDivLoss, self).__init__()

    def forward(self, p, q):
        p = F.softmax(p, dim=-1)
        q = F.softmax(q, dim=-1)
        loss = F.kl_div(q.log(), p, reduction='batchmean')
        return loss

用于记录两个分布之间的差距。


其中 reduction用于说明是否在batch上做平均:
batchmean:  如果batch_size = 32，则/32
none:不进行平均
sum:结果相加
mean:按照数组大小做平均



https://blog.csdn.net/qq_39450134/article/details/121745209